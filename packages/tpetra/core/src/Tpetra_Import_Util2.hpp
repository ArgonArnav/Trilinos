// @HEADER
// ***********************************************************************
//
//          Tpetra: Templated Linear Algebra Services Package
//                 Copyright (2008) Sandia Corporation
//
// Under the terms of Contract DE-AC04-94AL85000 with Sandia Corporation,
// the U.S. Government retains certain rights in this software.
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions are
// met:
//
// 1. Redistributions of source code must retain the above copyright
// notice, this list of conditions and the following disclaimer.
//
// 2. Redistributions in binary form must reproduce the above copyright
// notice, this list of conditions and the following disclaimer in the
// documentation and/or other materials provided with the distribution.
//
// 3. Neither the name of the Corporation nor the names of the
// contributors may be used to endorse or promote products derived from
// this software without specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY SANDIA CORPORATION "AS IS" AND ANY
// EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
// PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL SANDIA CORPORATION OR THE
// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
//
// Questions? Contact Michael A. Heroux (maherou@sandia.gov)
//
// ************************************************************************
// @HEADER

#ifndef TPETRA_IMPORT_UTIL2_HPP
#define TPETRA_IMPORT_UTIL2_HPP

///
/// \file Tpetra_Import_Util2.hpp
/// \brief Utility functions for packing and unpacking sparse matrix entries.
///

#include "Tpetra_ConfigDefs.hpp"
#include "Tpetra_Import.hpp"
#include "Tpetra_HashTable.hpp"
#include "Tpetra_Map.hpp"
#include "Tpetra_Util.hpp"
#include "Tpetra_Distributor.hpp"
#include "Tpetra_Details_reallocDualViewIfNeeded.hpp"
#include "Tpetra_Vector.hpp"
#include "Tpetra_Experimental_BlockCrsMatrix_def.hpp"
#include "Kokkos_DualView.hpp"
#include <Teuchos_Array.hpp>
#include <utility>
#include <unistd.h>
#include <tuple>

namespace Tpetra {
namespace Import_Util {

/// \brief Sort the entries of the (raw CSR) matrix by column index
///   within each row.
template<typename Scalar, typename Ordinal>
void
sortCrsEntries (const Teuchos::ArrayView<size_t>& CRS_rowptr,
                const Teuchos::ArrayView<Ordinal>& CRS_colind,
                const Teuchos::ArrayView<Scalar>&CRS_vals);

template<typename Ordinal>
void
sortCrsEntries (const Teuchos::ArrayView<size_t>& CRS_rowptr,
                const Teuchos::ArrayView<Ordinal>& CRS_colind);

template<typename rowptr_array_type, typename colind_array_type, typename vals_array_type>
void
sortCrsEntries (const rowptr_array_type& CRS_rowptr,
                const colind_array_type& CRS_colind,
                const vals_array_type& CRS_vals);

template<typename rowptr_array_type, typename colind_array_type>
void
sortCrsEntries (const rowptr_array_type& CRS_rowptr,
                const colind_array_type& CRS_colind);

/// \brief Sort and merge the entries of the (raw CSR) matrix by
///   column index within each row.
///
/// Entries with the same column index get merged additively.
template<typename Scalar, typename Ordinal>
void
sortAndMergeCrsEntries (const Teuchos::ArrayView<size_t>& CRS_rowptr,
                        const Teuchos::ArrayView<Ordinal>& CRS_colind,
                        const Teuchos::ArrayView<Scalar>& CRS_vals);

template<typename Ordinal>
void
sortAndMergeCrsEntries (const Teuchos::ArrayView<size_t>& CRS_rowptr,
                        const Teuchos::ArrayView<Ordinal>& CRS_colind);

/// \brief lowCommunicationMakeColMapAndReindex
///
/// If you know the owning PIDs already, you can make the colmap a lot
/// less expensively.  If LocalOrdinal and GlobalOrdinal are the same,
/// you can (and should) use the same array for both columnIndices_LID
/// and columnIndices_GID.  This routine works just fine "in place."
///
/// Note: The owningPids vector (on input) should contain owning PIDs
/// for each entry in the matrix, like that generated by
/// Tpetra::Import_Util::unpackAndCombineIntoCrsArrays routine.  Note:
/// This method will return a Teuchos::Array of the remotePIDs, used for
/// construction of the importer.
///
/// \warning This method is intended for expert developer use only,
///   and should never be called by user code.
template <typename LocalOrdinal, typename GlobalOrdinal, typename Node>
void
lowCommunicationMakeColMapAndReindex (const Teuchos::ArrayView<const size_t> &rowPointers,
                                      const Teuchos::ArrayView<LocalOrdinal> &columnIndices_LID,
                                      const Teuchos::ArrayView<GlobalOrdinal> &columnIndices_GID,
                                      const Teuchos::RCP<const Tpetra::Map<LocalOrdinal,GlobalOrdinal,Node> > & domainMap,
                                      const Teuchos::ArrayView<const int> &owningPids,
                                      Teuchos::Array<int> &remotePids,
                                      Teuchos::RCP<const Tpetra::Map<LocalOrdinal,GlobalOrdinal,Node> > & colMap);




  /// \brief Generates an list of owning PIDs based on two transfer (aka import/export objects)
  /// Let:
  ///   OwningMap = useReverseModeForOwnership ? transferThatDefinesOwnership.getTargetMap() : transferThatDefinesOwnership.getSourceMap();
  ///   MapAo     = useReverseModeForOwnership ? transferThatDefinesOwnership.getSourceMap() : transferThatDefinesOwnership.getTargetMap();
  ///   MapAm     = useReverseModeForMigration ? transferThatDefinesMigration.getTargetMap() : transferThatDefinesMigration.getSourceMap();
  ///   VectorMap = useReverseModeForMigration ? transferThatDefinesMigration.getSourceMap() : transferThatDefinesMigration.getTargetMap();
  /// Precondition:
  ///  1) MapAo.isSameAs(*MapAm)                      - map compatibility between transfers
  ///  2) VectorMap->isSameAs(*owningPIDs->getMap())  - map compabibility between transfer & vector
  ///  3) OwningMap->isOneToOne()                     - owning map is 1-to-1
  ///  --- Precondition 3 is only checked in DEBUG mode ---
  /// Postcondition:
  ///   owningPIDs[VectorMap->getLocalElement(GID i)] =   j iff  (OwningMap->isLocalElement(GID i) on rank j)
  template <typename LocalOrdinal, typename GlobalOrdinal, typename Node>
  void getTwoTransferOwnershipVector(const ::Tpetra::Details::Transfer<LocalOrdinal, GlobalOrdinal, Node>& transferThatDefinesOwnership,
                                     bool useReverseModeForOwnership,
                                     const ::Tpetra::Details::Transfer<LocalOrdinal, GlobalOrdinal, Node>& transferForMigratingData,
                                     bool useReverseModeForMigration,
                                     Tpetra::Vector<int,LocalOrdinal,GlobalOrdinal,Node> & owningPIDs);

} // namespace Import_Util
} // namespace Tpetra


//
// Implementations
//

namespace Tpetra {
namespace Import_Util {


template<typename PID, typename GlobalOrdinal>
bool sort_PID_then_GID(const std::pair<PID,GlobalOrdinal> &a,
                       const std::pair<PID,GlobalOrdinal> &b)
{
  if(a.first!=b.first)
    return (a.first < b.first);
  return (a.second < b.second);
}

template<typename PID,
         typename GlobalOrdinal,
         typename LocalOrdinal>
bool sort_PID_then_pair_GID_LID(const std::pair<PID, std::pair< GlobalOrdinal, LocalOrdinal > > &a,
                                const std::pair<PID, std::pair< GlobalOrdinal, LocalOrdinal > > &b)
{
  if(a.first!=b.first)
    return a.first < b.first;
  else
    return (a.second.first < b.second.first);
}



// template<typename PID,
//          typename LocalOrdinal,
//          typename GlobalOrdinal,
//          typename Node>
// void 
// relayGIDOwner (Teuchos::RCP<const Tpetra::Map<LocalOrdinal,GlobalOrdinal,Node> > MyDomainMap,
//                const Teuchos::Array<std::pair<PID,GlobalOrdinal>> & vpg,
//                const Teuchos::Array<PID> ProcsFrom,
//                Teuchos::Array<std::tuple<PID,PID,GlobalOrdinal> > & relayedGIDs )  {
// // When passed a list of pair<pid,gid>, check if the GID is for a local element, if not
// // send it to the PID. When received, return the PID of the sending process with the GID.

//   typedef std::pair<PID,GlobalOrdinal> pidgidpair_t;

//   auto const comm             = MyDomainMap->getComm();
//   MPI_Comm rawComm = getRawMpiComm(*comm);

//   const LocalOrdinal INVALIDLID = Teuchos::OrdinalTraits<LocalOrdinal>::invalid();
//   std::multimap<PID,GlobalOrdinal> relaymap;
//   std::set<PID> upid;
//   for ( auto && p : vpg)  {
//     PID pid;
//     GlobalOrdinal gid;
//     std::tie(pid,gid) = p;
//     if(MyDomainMap->getLocalElement(gid) == INVALIDLID) {
//       relaymap.insert(p);
//       upid.insert(pid);
//     }
//   }
//   Teuchos::Array<Teuchos::Array<pidgidpair_t> > rdata(upid.size());
//   int ss=0;
//   for( auto && up : upid) {
//     auto itlow = relaymap.lower_bound(up);
//     auto itup  = relaymap.upper_bound(up);
//     for( auto it = itlow;it!=itup;++it) 
//       rdata[ss].push_back(*it);
//     ss++;
//   }
//   Teuchos::Array<int> SizeProcsFrom(ProcsFrom.size(),-1);
//   Teuchos::Array<MPI_Request> rawRreq(ProcsFrom.size(), MPI_REQUEST_NULL);
//   Teuchos::Array<MPI_Request> rawSreq(upid.size(), MPI_REQUEST_NULL);

//   // exchange sizes
//  for(int i=0;i<ProcsFrom.size();++i) {

//    MPI_Request rawRequest = MPI_REQUEST_NULL;
//    MPI_Irecv(&SizeProcsFrom[i],
//              1,
//              MPI_INT,
//              ProcsFrom[i],
//              0,
//              rawComm,
//              &rawRequest);
//    rawRreq[i]=rawRequest;
//   }

//  int s=0; 
//  for( auto && up : upid) {
//    const int sendsize = rdata[s].size();
  
//    MPI_Request rawRequest = MPI_REQUEST_NULL;
//    MPI_Isend(&sendsize,
//              1,
//              MPI_INT,
//              up,
//              0,
//              rawComm,
//              &rawRequest);
//    rawSreq[s]=rawRequest; // not used. 
//    s++;
//  }
 
//  Teuchos::Array<MPI_Status> rawRstatus(rawRreq.size());
//  int err = MPI_Waitall (rawRreq.size(), rawRreq.getRawPtr(),
//                         rawRstatus.getRawPtr());

//   Teuchos::Array<Teuchos::Array<pidgidpair_t> > recvBuff(ProcsFrom.size());
//   for(uint i=0;i<ProcsFrom.size();++i ) recvBuff[i].resize(SizeProcsFrom[i]);
 
//   for(int i=0;i<ProcsFrom.size();++i) {
//     char * myrecv  = (char *) recvBuff[i].getRawPtr();
//     MPI_Request rawRequest = MPI_REQUEST_NULL;
//     MPI_Irecv(const_cast<char*>(myrecv),
//               SizeProcsFrom[i]*sizeof(pidgidpair_t),
//               MPI_CHAR,
//               ProcsFrom[i],
//               0,
//               rawComm,
//               &rawRequest);
//     rawRreq[i]=rawRequest;
//   }
//   s=0;
//   for(auto && up : upid) {
//     char * mysend =  (char *)(rdata[s].getRawPtr()); 
//     int datacount = rdata[s].size();
//     MPI_Request rawRequest = MPI_REQUEST_NULL;

//     MPI_Isend(mysend,
//               sizeof(pidgidpair_t)*datacount,
//               MPI_CHAR,
//               up,
//               0,
//               rawComm,
//               &rawRequest);
//     rawSreq[s]=rawRequest;
//     ++s;
//   }

//   err = MPI_Waitall (rawRreq.size(), rawRreq.getRawPtr(),
//                                rawRstatus.getRawPtr());
  
//   TEUCHOS_TEST_FOR_EXCEPTION( err != 0, std::runtime_error," relayGIDOwner died in second MPI_Waitall" );
//   relayedGIDs.clear();

//   for(int i=0;i<ProcsFrom.size();++i) 
//     for( auto && g : recvBuff[i]) {
//       GlobalOrdinal gid;
//       PID rp;
//       std::tie(rp,gid) = g;
//       relayedGIDs.push_back(std::make_tuple (ProcsFrom[i],rp,gid));
//     }
//  }



template<typename Scalar,
         typename LocalOrdinal,
         typename GlobalOrdinal,
         typename Node>
void
rev_ND(const CrsMatrix<Scalar, LocalOrdinal, GlobalOrdinal, Node>  & SourceMatrix,
       Teuchos::ArrayView<const LocalOrdinal> ExportLIDs,
       Teuchos::ArrayView<const int> ExportPIDs,
       Teuchos::ArrayView<const GlobalOrdinal> tgtRemoteGIDs,
       Teuchos::RCP<const Tpetra::Import<LocalOrdinal,GlobalOrdinal,Node> > MyImporter,
       Teuchos::RCP<const Tpetra::Map<LocalOrdinal,GlobalOrdinal,Node> > MyDomainMap,
       Teuchos::Array<int>& reversePIDs,
       Teuchos::Array<LocalOrdinal>& reverseLIDs,
       Teuchos::Array<GlobalOrdinal>& reverseGIDs,
       Teuchos::Array<std::pair<int,GlobalOrdinal> > & fromRemoteGID)

{
 // need to bring in the target row map, as I don't want to replicate the logic for forward/reverse
  // and reduced rowMap that is in the exportAndFillComplete routine.

  std::string pfx = std::string(" Import_Util2::rev_ND::");

  typedef LocalOrdinal LO;
  typedef GlobalOrdinal GO;
  typedef std::pair<int,GlobalOrdinal> pidgidpair_t;
  using Teuchos::RCP;

  auto const comm             = MyDomainMap->getComm();
  const int MyPID             = comm->getRank ();

  auto const NumExportLIDs              = ExportLIDs.size();

  
  TEUCHOS_TEST_FOR_EXCEPTION(MyImporter.is_null(),
			     std::logic_error, 
			     "Tpetra::reverseNeighborDiscovery "
			     "Neighbor Discovery Should not be called with null Importer");

  Distributor & Distor            = MyImporter->getDistributor();
  auto const NumRecvs                   = Distor.getNumReceives();
  auto const NumSends                   = Distor.getNumSends();
  auto RemoteLIDs                 = MyImporter->getRemoteLIDs();
  Teuchos::ArrayView<const int> ProcsFrom       = Distor.getProcsFrom();
  Teuchos::ArrayView<const int> ProcsTo          = Distor.getProcsTo();
  Teuchos::ArrayView<const int> LengthsFrom     = Distor.getLengthsFrom();
  auto MyColMap                   = SourceMatrix.getColMap();
  const size_t numCols            = MyColMap->getNodeNumElements ();
  RCP<const Tpetra::Map<LocalOrdinal,GlobalOrdinal,Node> > target = MyImporter->getTargetMap();

  // ProcsFrom[RemotePIDs[i]] is the proc that owns RemoteLIDs[j]....
  Teuchos::Array<int> RemotePIDOrder(numCols,-1);

  // For each remote ID, record index into ProcsFrom, who owns it.
  for(uint i=0,j=0;i<NumRecvs;i++){
    for(uint k=0;k<LengthsFrom[i];k++){
      int pid=ProcsFrom[i];
      if(pid!=MyPID ) RemotePIDOrder[RemoteLIDs[j]]=i;
      j++;
    }
  }

  // Step One: Start tacking the (GID,PID) pairs on the reserved arrays
  //
  // For each index in ProcsFrom, we will insert into a set of (PID, GID) 
  // pairs, in order to build a list of such pairs for each of
  // those processes.  Since this is building a reverse, we will send
  // to these processes.
  std::vector<std::vector<pidgidpair_t> > ReversePGIDs(NumRecvs); // needs to be size of ProcsFrom
  for(auto & v : ReversePGIDs) v.reserve(NumExportLIDs);

  Teuchos::ArrayRCP<const size_t> rowptr;
  Teuchos::ArrayRCP<const LO> colind;
  Teuchos::ArrayRCP<const Scalar> vals;

  SourceMatrix.getAllValues(rowptr,colind,vals);  // is there a more efficient way to get just rowptr?

  // Loop over each exported row and add to the temp list
  for(size_t i=0; i < NumExportLIDs; i++) {
    LO lid = ExportLIDs[i];
    GO exp_pid = ExportPIDs[i];
    for(auto j=rowptr[lid]; j<rowptr[lid+1]; j++){
      int pid_order = RemotePIDOrder[colind[j]];
      if(pid_order!=-1) {
        GO gid = MyColMap->getGlobalElement(colind[j]);  //Epetra SM.GCID46 =>sm->graph-> {colmap(colind)}
        ReversePGIDs[pid_order].push_back(pidgidpair_t(exp_pid,gid));
      }
    }
  }
  
// DEBUG only. 
//  for(auto & v : ReversePGIDs) assert(v.size() == NumExportLIDs);

  // Step 2: Count sizes
  Teuchos::Array<std::pair<size_t,size_t>> ReverseSendSizes(NumRecvs);
  for(size_t j = 0; j<ReversePGIDs.size(); ++j) {
    ReverseSendSizes[j].first = ReversePGIDs[j].size();
    ReverseSendSizes[j].second = tgtRemoteGIDs.size();
  }

// CBL: Do the send/receive buffer need to have <pid,gid> or just <gid> to save space(?)

  // Step 3: Alloc and fill the send buffer
  Teuchos::Array<Teuchos::Array<pidgidpair_t > > ReverseSendBuffer(NumRecvs);
  Teuchos::Array<Teuchos::Array<pidgidpair_t > > ReverseRecvBuffer(NumSends);

  for(uint i=0;i<NumRecvs;++i)
    ReverseSendBuffer[i].reserve(ReverseSendSizes[i].first + ReverseSendSizes[i].second); 

  uint pididx=0;
  for(auto&& vecofPairs : ReversePGIDs) {
    if( pididx >= NumRecvs) {
      std::cerr<<"  size missmatch in Import_Util::reverseNeighborDiscovery "<<std::endl;
      MPI_Abort (MPI_COMM_WORLD, -1);
    }
    for(auto&& pidGidPair : vecofPairs) {
      ReverseSendBuffer[pididx].push_back(pidGidPair);
    }
    // now tack on tgtRemoteGIDs: Note it would be more space efficient to omit MyPID, and just pack the tgtRemoteGIDs
    for(auto&& remGID : tgtRemoteGIDs) {
      ReverseSendBuffer[pididx].push_back(pidgidpair_t(MyPID,remGID));
    }
    pididx++;
  }

  Teuchos::Array<std::pair<size_t,size_t>> ReverseRecvSizes(NumSends);
  Teuchos::Array<Teuchos::RCP<Teuchos::CommRequest<int> > > mpireq;
  Teuchos::Array<Teuchos::RCP<Teuchos::CommRequest<int> > > mpisend;
  RCP<Teuchos::CommRequest<int> > recvRequest;
  RCP<Teuchos::CommRequest<int> > sendRequest;

  for(int i=0;i<ProcsTo.size();++i) {
    recvRequest = Teuchos::ireceive<int,std::pair<size_t,size_t> >(*comm,Teuchos::rcp(&ReverseRecvSizes[i],false),ProcsTo[i]);
    mpireq.push_back(recvRequest);

  }
  for(int i=0;i<ProcsFrom.size();++i) { 
    sendRequest = Teuchos::isend<int,std::pair<size_t,size_t> >(*comm,Teuchos::rcp(&ReverseSendSizes[i],false),ProcsFrom[i]);
    mpisend.push_back(sendRequest);
  }
  Teuchos::waitAll(*comm,mpireq());
  MPI_Comm rawComm = getRawMpiComm(*comm);

  Teuchos::Array<MPI_Request> rawRreq(ProcsTo.size(), MPI_REQUEST_NULL);
  Teuchos::Array<MPI_Request> rawSreq(ProcsFrom.size(), MPI_REQUEST_NULL);


  std::vector<int> rsize1(ProcsTo.size(),-1);
  std::vector<int> rsize2(ProcsTo.size(),-1);
  
   for(int i=0;i<ProcsTo.size();++i) {
       char * thisrec = (char *) (&rsize1[i]);
       MPI_Request rawRequest = MPI_REQUEST_NULL;
       MPI_Irecv(const_cast<char*>(thisrecv),
		 sizeof(int),
		 MPI_CHAR,
		 ProcsTo[i],
		 0,
		 rawComm,
		 &rawRequest);
       rawRreq[i]=rawRequest;
   }
   for(int i=0;i<ProcsFrom.size();++i) { 
       char * mysend =  (char *)(&ReverseSendSize[i].first);
       MPI_Request rawRequest = MPI_REQUEST_NULL;
       MPI_Isend(mysend,
		 sizeof(int),
		 MPI_CHAR,
		 ProcsFrom[i],
		 0,
		 rawComm,
		 &rawRequest);
       rawSreq[i]=rawRequest;
   }
   for(int i=0;i<ProcsTo.size();++i) {
       char * thisrec = (char *) (&rsize2[i]);
       MPI_Request rawRequest = MPI_REQUEST_NULL;
       MPI_Irecv(const_cast<char*>(thisrecv),
		 sizeof(int),
		 MPI_CHAR,
		 ProcsTo[i],
		 0,
		 rawComm,
		 &rawRequest);
       rawRreq[i]=rawRequest;
   }
   for(int i=0;i<ProcsFrom.size();++i) { 
       char * mysend =  (char *)(&ReverseSendSize[i].second);
       MPI_Request rawRequest = MPI_REQUEST_NULL;
       MPI_Isend(mysend,
		 sizeof(int),
		 MPI_CHAR,
		 ProcsFrom[i],
		 0,
		 rawComm,
		 &rawRequest);
       rawSreq[i]=rawRequest;
   }

   std::ostringstream serr;
   bool sizeerr=false;
   for(int i=0;i<ProcsTo.size();++i) {

       if(rsize1[i]!=ReverseRecvSize[i].first){
	   sizeerr=true;
	   serr<<MyPID<<" Mismatch in received 1st sizes between Teuchos::isend and raw MPI send Raw:"<<rsize1[i]<<" T::isend "<<ReverseRecvSize[i].first<<std::endl;
       }
       if(rsize2[i]!=ReverseRecvSize[i].second){
	   sizeerr=true;
	   serr<<MyPID<<" Mismatch in received 2nd sizes between Teuchos::isend and raw MPI send Raw:"<<rsize2[i]<<" T::isend "<<ReverseRecvSize[i].second<<std::endl;
       }
   }
   if(sizeerr) std::cerr<<serr.str()<<std::flush<<std::flush;

   comm->barrier();
   comm->barrier();
   if(sizeerr) {
       std::cerr<<std::flush;
       comm->barrier();
       comm->barrier();
   }
   assert(!sizeerr);


  for(uint i=0;i<NumSends;++i)
    ReverseRecvBuffer[i].resize(ReverseRecvSizes[i].first+ReverseRecvSizes[i].second,pidgidpair_t(-9999,-8888));

  mpireq.clear();
  mpisend.clear();



  for(int i=0;i<ProcsTo.size();++i) {
    char * myrecv  = (char *) (&ReverseRecvBuffer[i])->getRawPtr();
    MPI_Request rawRequest = MPI_REQUEST_NULL;
    MPI_Irecv(const_cast<char*>(myrecv),
              ReverseRecvBuffer[i].size()*sizeof(pidgidpair_t),
              MPI_CHAR,
              ProcsTo[i],
              0,
              rawComm,
              &rawRequest);
    rawRreq[i]=rawRequest;
  }
  for(int i=0;i<ProcsFrom.size();++i) { 
    char * mysend =  (char *)(&ReverseSendBuffer[i])->getRawPtr();
    MPI_Request rawRequest = MPI_REQUEST_NULL;

    if(ReverseSendBuffer[i].size()!=ReverseSendSizes[i].first + ReverseSendSizes[i].second)
    {
     std::ostringstream os;
     os << "reverseNeighborDiscovery Send of ReverseSendBuffer["<<i<<"] size missmatch "<< " buff.size() "<<ReverseSendBuffer[i].size()<<" != first+second: "<<ReverseSendSizes[i].first<<"  + "<<ReverseSendSizes[i].second<<std::endl;
     std::cerr<<os.str()<<std::flush;
     MPI_Abort (MPI_COMM_WORLD, -1);
    } 


    MPI_Isend(mysend,
              sizeof(pidgidpair_t)*(ReverseSendSizes[i].first + ReverseSendSizes[i].second),
              MPI_CHAR,
              ProcsFrom[i],
              0,
              rawComm,
              &rawRequest);
    rawSreq[i]=rawRequest;
  }

  Teuchos::Array<MPI_Status> rawRstatus(rawRreq.size());

  const int err = MPI_Waitall (rawRreq.size(), rawRreq.getRawPtr(),
                               rawRstatus.getRawPtr());
  if(err) {
     std::ostringstream os;
     os << "reverseNeighborDiscovery Mpi_Waitall error on receive ";
     os<<std::endl;
     std::cerr<<os.str()<<std::flush;
     assert(false);
     MPI_Abort (MPI_COMM_WORLD, -1);
  }

  size_t totalexportpairrecsize = 0;
  for(size_t i = 0; i < (size_t)ReverseRecvSizes.size(); i++) {
    totalexportpairrecsize += ReverseRecvSizes[i].first;
    if(ReverseRecvSizes[i].first<0) {
	std::ostringstream os;
	os << MyPID << "reverseNeighborDiscovery: got a -1 for receive size "<<std::endl;
	std::cerr<<os.str()<<std::flush;
	assert(false);
	MPI_Abort (MPI_COMM_WORLD, -1);
    }
  }

  // Sort RecvBuffer first by PID, then by GID.
  Teuchos::Array<pidgidpair_t > AllReverseRecv;
  AllReverseRecv.reserve(totalexportpairrecsize);
  
  size_t totalforeignremotesize =0;
  for(auto &&p : ReverseRecvSizes) {
      totalforeignremotesize+=p.second;
      if(p.second<0) {
	  std::ostringstream os;
	  os << MyPID << "reverseNeighborDiscovery: got a -1 for foreignremote receive size "<<std::endl;
	  std::cerr<<os.str()<<std::flush;
	  assert(false);
	  MPI_Abort (MPI_COMM_WORLD, -1);
      }
  }

  fromRemoteGID.clear();
  fromRemoteGID.reserve(totalforeignremotesize);

  for(uint i=0;i<ReverseRecvBuffer.size();++i) {
    // for(size_t s = 0; s< ReverseRecvSizes[i].first ; ++s) // first reverse export pairs
    // 	AllReverseRecv.push_back(ReverseRecvBuffer[i][s]);
      
      AllReverseRecv.insert(AllReverseRecv.begin(),
			    ReverseRecvBuffer[i].begin(),
			    ReverseRecvBuffer[i].begin()+ReverseRecvSizes[i].first);
      // this should work, test the below loop for debug. 
      // fromRemoteGID.insert(fromRemoteGID.begin(), 
      // 			   ReverseRecvBuffer[i].begin()+ReverseRecvSizes[i].first,
      // 			   ReverseRecvBuffer[i].end());

    for(size_t s=ReverseRecvSizes[i].first ; s<ReverseRecvSizes[i].first+ReverseRecvSizes[i].second; ++s) { 
    	fromRemoteGID.push_back(ReverseRecvBuffer[i][s]);

    	if(ReverseRecvBuffer[i][s].first != ProcsTo[i]) {  
    	    std::ostringstream os; 
    	    os <<MyPID<<" rev_ND::error in unpack RecvBuffPID "<<ReverseRecvBuffer[i][s].first<<" from "<<ProcsTo[i]<<" for buff index "<<i<<" of "<<ReverseRecvBuffer.size()<< std::endl;
    	    std::cerr<<os.str()<<std::flush;    
	    assert(false);
    	    MPI_Abort (MPI_COMM_WORLD, -1);
    	}
    }
  }

  if(AllReverseRecv.size() !=totalexportpairrecsize) {
      std::ostringstream os; 
      os <<MyPID<<" rev_ND::error in unpack RecvBuffPID AllReverseRecv.size!=totalexportpairrecsize "<<AllReverseRecv.size()<<" vs "<<totalexportpairrecsize<<std::endl;
        std::cerr<<os.str()<<std::flush;    
        MPI_Abort (MPI_COMM_WORLD, -1);
  }
  
  if(fromRemoteGID.size() !=totalforeignremotesize) {
      std::ostringstream os; 
      os <<MyPID<<" rev_ND::error in unpack RecvBuffPID fromRemoteGID.size!=totalforeignremotesize "<<fromRemoteGID.size()<<" vs "<<totalforeignremotesize<<std::endl;
        std::cerr<<os.str()<<std::flush;    
        MPI_Abort (MPI_COMM_WORLD, -1);
  }
  

  std::sort(AllReverseRecv.begin(), AllReverseRecv.end(), Tpetra::Import_Util::sort_PID_then_GID<int, GlobalOrdinal>);
//  std::sort(AllReverseRecv.begin(), AllReverseRecv.end());
  auto newEndOfPairs = std::unique(AllReverseRecv.begin(), AllReverseRecv.end());
  AllReverseRecv.resize( std::distance(AllReverseRecv.begin(),newEndOfPairs) );
  reversePIDs.clear();
  reverseLIDs.clear();
  reverseGIDs.clear();
  
  reversePIDs.reserve(AllReverseRecv.size());
  reverseLIDs.reserve(AllReverseRecv.size());
  reverseGIDs.reserve(AllReverseRecv.size());

  for(auto && pgPair : AllReverseRecv) {
    if((int)pgPair.first != MyPID) {
      // if this pair is not found in fromRemoteGID, it's bogus, skip it. 
      // if(std::find(fromRemoteGID.begin(),fromRemoteGID.end(),pgPair) == fromRemoteGID.end()) 
      //   {
      //     std::ostringstream os;
      //     os<<MyPID<<"  RND not found "<<pgPair.first<<" "<<pgPair.second<<std::endl;
      //     std::cerr<<os.str()<<std::flush;
      //     continue;
      //   }
      reversePIDs.push_back((int)pgPair.first);
      reverseGIDs.push_back(pgPair.second);
      LocalOrdinal lid = MyDomainMap->getLocalElement(pgPair.second);
      reverseLIDs.push_back(lid);
    }
  }
}



template<typename Scalar,
         typename LocalOrdinal,
         typename GlobalOrdinal,
         typename Node>
void
reverseNeighborDiscovery(const CrsMatrix<Scalar, LocalOrdinal, GlobalOrdinal, Node>  & SourceMatrix,
                         const Tpetra::Details::Transfer<LocalOrdinal,GlobalOrdinal,Node>& RowTransfer,
                         Teuchos::ArrayView<const GlobalOrdinal> tgtRemoteGIDs,
                         Teuchos::RCP<const Tpetra::Import<LocalOrdinal,GlobalOrdinal,Node> > MyImporter,
                         Teuchos::RCP<const Tpetra::Map<LocalOrdinal,GlobalOrdinal,Node> > MyDomainMap,
                         Teuchos::Array<int>& reversePIDs,
                         Teuchos::Array<LocalOrdinal>& reverseLIDs,
                         Teuchos::Array<GlobalOrdinal>& reverseGIDs,
                         Teuchos::Array<std::pair<int,GlobalOrdinal> > & fromRemoteGID,
                         Teuchos::RCP<const Teuchos::Comm<int> >& rcomm)
{
  // need to bring in the target row map, as I don't want to replicate the logic for forward/reverse
  // and reduced rowMap that is in the exportAndFillComplete routine.

  std::string pfx = std::string(" Import_Util2::pAPRC:: ");

  std::ostringstream errstr;
  bool error = false; 

  typedef LocalOrdinal LO;
  typedef GlobalOrdinal GO;
  typedef std::pair<int,GlobalOrdinal> pidgidpair_t;
  using Teuchos::RCP;

  auto const comm             = MyDomainMap->getComm();
  const int MyPID             = rcomm->getRank ();

  // Things related to messages I am sending in forward mode (RowTransfer)
  // *** Note: this will be incorrect for transferAndFillComplete if it is in reverse mode. FIXME cbl.
  auto ExportPIDs                 = RowTransfer.getExportPIDs();
  auto ExportLIDs                 = RowTransfer.getExportLIDs();
  auto NumExportLIDs              = RowTransfer.getNumExportIDs();

  
  TEUCHOS_TEST_FOR_EXCEPTION(MyImporter.is_null(),
			     std::logic_error, 
			     "Tpetra::reverseNeighborDiscovery "
			     "Neighbor Discovery Should not be called with null Importer");

  Distributor & Distor            = MyImporter->getDistributor();
  auto NumRecvs                   = Distor.getNumReceives();
  auto NumSends                   = Distor.getNumSends();
  auto RemoteLIDs                 = MyImporter->getRemoteLIDs();
  Teuchos::ArrayView<const int> ProcsFrom       = Distor.getProcsFrom();
  auto const ProcsTo              = Distor.getProcsTo();
  auto LengthsFrom                = Distor.getLengthsFrom();
  auto MyColMap                   = SourceMatrix.getColMap();
  const size_t numCols            = MyColMap->getNodeNumElements ();
  RCP<const Tpetra::Map<LocalOrdinal,GlobalOrdinal,Node> > target = MyImporter->getTargetMap();

  // Get the owning pids in a special way,
  // s.t. ProcsFrom[RemotePIDs[i]] is the proc that owns RemoteLIDs[j]....
  Teuchos::Array<int> RemotePIDOrder(numCols,-1);

  // For each remote ID, record index into ProcsFrom, who owns it.
  for(uint i=0,j=0;i<NumRecvs;i++){
    for(uint k=0;k<LengthsFrom[i];k++){
      int pid=ProcsFrom[i];
      if(pid!=MyPID ) RemotePIDOrder[RemoteLIDs[j]]=i;
      j++;
    }
  }

  // Step One: Start tacking the (GID,PID) pairs on the std sets
  //
  // For each index in ProcsFrom, we will insert into a set of (PID,
  // GID) pairs, in order to build a list of such pairs for each of
  // those processes.  Since this is building a reverse, we will send
  // to these processes.
  std::vector<std::vector<pidgidpair_t> > ReversePGIDs(NumRecvs); // needs to be size of ProcsFrom
  for(auto & v : ReversePGIDs) v.reserve(NumExportLIDs);

  Teuchos::ArrayRCP<const size_t> rowptr;
  Teuchos::ArrayRCP<const LO> colind;
  Teuchos::ArrayRCP<const Scalar> vals;

  SourceMatrix.getAllValues(rowptr,colind,vals);  // is there a more efficient way to get just rowptr?

  // Loop over each exported row and add to the temp list
  // note that if NumExportLIDs is 0, ExportLIDs.is_null()==true
  for(size_t i=0; i < NumExportLIDs; i++) {
    LO lid = ExportLIDs[i];
    GO exp_pid = ExportPIDs[i];
    for(auto j=rowptr[lid]; j<rowptr[lid+1]; j++){
      int pid_order = RemotePIDOrder[colind[j]];
      if(pid_order!=-1) {
        GO gid = MyColMap->getGlobalElement(colind[j]);  //Epetra SM.GCID46 =>sm->graph-> {colmap(colind)}
        ReversePGIDs[pid_order].push_back(pidgidpair_t(exp_pid,gid));
      }
    }
  }
  
  // Step 2: Count sizes
  Teuchos::Array<std::pair<int,int>> ReverseSendSizes(NumRecvs);
  for(size_t j = 0; j<ReversePGIDs.size(); ++j) {
    ReverseSendSizes[j].first = ReversePGIDs[j].size();
    ReverseSendSizes[j].second = tgtRemoteGIDs.size();
  }

  // Step 3: Alloc and fill the send buffer
  Teuchos::Array<Teuchos::Array<pidgidpair_t > > ReverseSendBuffer(NumRecvs);
  Teuchos::Array<Teuchos::Array<pidgidpair_t > > ReverseRecvBuffer(NumSends);

  for(uint i=0;i<NumRecvs;++i)
    ReverseSendBuffer[i].reserve(ReverseSendSizes[i].first + ReverseSendSizes[i].second); 

  uint pididx=0;
  for(auto&& vecofPairs : ReversePGIDs) {
    if( pididx >= NumRecvs) {
      errstr<<" E1 size missmatch in Import_Util::reverseNeighborDiscovery "<<std::endl;
      error = true;
    }
    for(auto&& pidGidPair : vecofPairs) {
      ReverseSendBuffer[pididx].push_back(pidGidPair);
    }
    // now tack on tgtRemoteGIDs: Note it would be more space efficient to omit MyPID, and just pack the tgtRemoteGIDs
    for(auto&& remGID : tgtRemoteGIDs) {
      ReverseSendBuffer[pididx].push_back(pidgidpair_t(MyPID,remGID));
    }
    pididx++;
  }

  Teuchos::Array<std::pair<int,int>> ReverseRecvSizes(NumSends);
  for(int i=0;i<NumSends;++i) ReverseRecvSizes[i] = std::pair<int,int>(-1,-1);

  Teuchos::Array<Teuchos::RCP<Teuchos::CommRequest<int> > > mpireq;
  Teuchos::Array<Teuchos::RCP<Teuchos::CommRequest<int> > > mpisend;
  RCP<Teuchos::CommRequest<int> > recvRequest;
  RCP<Teuchos::CommRequest<int> > sendRequest;
  for(int i=0;i<ProcsTo.size();++i) {
    recvRequest = Teuchos::ireceive<int,std::pair<int,int> >(*comm,Teuchos::rcp(&ReverseRecvSizes[i],false),ProcsTo[i]);
    mpireq.push_back(recvRequest);

  }
  for(int i=0;i<ProcsFrom.size();++i) { 
    sendRequest = Teuchos::isend<int,std::pair<int,int> >(*comm,Teuchos::rcp(&ReverseSendSizes[i],false),ProcsFrom[i]);
    mpisend.push_back(sendRequest);
  }

  Teuchos::waitAll(*comm,mpireq());
 
  size_t totalexportpairrecsize = 0;
  for(size_t i = 0; i < (size_t)ReverseRecvSizes.size(); i++) {
    totalexportpairrecsize += ReverseRecvSizes[i].first;
    if(ReverseRecvSizes[i].first<0) {
	errstr << MyPID << "E4 reverseNeighborDiscovery: got a -1 for receive size "<<std::endl;
	error=true;
    }
     if(ReverseRecvSizes[i].second<0) {
	errstr << MyPID << "E4.1 reverseNeighborDiscovery: got a -1 for receive size "<<std::endl;
	error=true;
    }
  }

  for(uint i=0;i<NumSends;++i)
    ReverseRecvBuffer[i].resize(ReverseRecvSizes[i].first+ReverseRecvSizes[i].second,pidgidpair_t(-9999,-8888));

  mpireq.clear();
  mpisend.clear();

  Teuchos::Array<MPI_Request> rawRreq(ProcsTo.size(), MPI_REQUEST_NULL);
  Teuchos::Array<MPI_Request> rawSreq(ProcsFrom.size(), MPI_REQUEST_NULL);

  MPI_Comm rawComm = getRawMpiComm(*comm);

  for(int i=0;i<ProcsTo.size();++i) {
    char * myrecv  = (char *) (&ReverseRecvBuffer[i])->getRawPtr();
    MPI_Request rawRequest = MPI_REQUEST_NULL;
    MPI_Irecv(const_cast<char*>(myrecv),
              ReverseRecvBuffer[i].size()*sizeof(pidgidpair_t),
              MPI_CHAR,
              ProcsTo[i],
              0,
              rawComm,
              &rawRequest);
    rawRreq[i]=rawRequest;
  }
  for(int i=0;i<ProcsFrom.size();++i) { 
    char * mysend =  (char *)(&ReverseSendBuffer[i])->getRawPtr();
    MPI_Request rawRequest = MPI_REQUEST_NULL;

    if(ReverseSendBuffer[i].size()!=ReverseSendSizes[i].first + ReverseSendSizes[i].second)
    {
     errstr << "E2 reverseNeighborDiscovery Send of ReverseSendBuffer["<<i<<"] size missmatch "<< " buff.size() "<<ReverseSendBuffer[i].size()<<" != first+second: "<<ReverseSendSizes[i].first<<"  + "<<ReverseSendSizes[i].second<<std::endl;
     error=true;
     std::cerr<<errstr.str()<<std::flush;
     std::cerr<<std::flush<<std::flush;
    } 


    MPI_Isend(mysend,
              ReverseSendBuffer[i].size()*sizeof(pidgidpair_t),
              MPI_CHAR,
              ProcsFrom[i],
              0,
              rawComm,
              &rawRequest);
    rawSreq[i]=rawRequest;
  }

  Teuchos::Array<MPI_Status> rawRstatus(rawRreq.size());

  const int err = MPI_Waitall (rawRreq.size(), rawRreq.getRawPtr(),
                               rawRstatus.getRawPtr());
  if(err) {
      errstr <<MyPID<< "E3 reverseNeighborDiscovery Mpi_Waitall error on receive ";
     error=true;
  }

  const int comsize = comm->getSize();
  // DEBUG ONLY 
  bool badpid = false;
  for(int i=0;i<ProcsTo.size();++i) {

      int pos =0;
      for( auto && p : ReverseRecvBuffer[i]) {

	  if(p.first < 0 || p.first > comsize ) {

	      errstr<<MyPID<<" received from "<<ProcsTo[i]<<" a bad pid of "<<p.first<< " pos = "<<pos<<" into reverse buffer, with sizes "<< ReverseRecvSizes[i].first<<" "<<ReverseRecvSizes[i].second<<std::endl;
	      badpid = true;
	  }
	  pos++;
      }
  }

  if(badpid) {
      std::cerr<<errstr.str()<<std::flush;
      comm->barrier();
      comm->barrier();
      comm->barrier();
      MPI_Abort (MPI_COMM_WORLD, -1);
  }

  // Sort RecvBuffer first by PID, then by GID.
  Teuchos::Array<pidgidpair_t > AllReverseRecv;
  AllReverseRecv.reserve(totalexportpairrecsize);
  
  size_t totalforeignremotesize =0;
  for(auto &&p : ReverseRecvSizes) {
      totalforeignremotesize+=p.second;
      if(p.second<0) {
	  errstr<< MyPID << "E5 reverseNeighborDiscovery: got a -1 for foreignremote receive size "<<std::endl;
	  error=true;
      }
  }

  fromRemoteGID.clear();
  fromRemoteGID.reserve(totalforeignremotesize);

  for(uint i=0;i<ReverseRecvBuffer.size();++i) {
    // AllReverseRecv.insert( AllReverseRecv.end(),
    // 			   ReverseRecvBuffer[i].begin(),
    // 			   ReverseRecvBuffer[i].begin()+ReverseRecvSizes[i].first);
//    s = ReverseRecvSizes[i].first;
    for(size_t s = 0; s< ReverseRecvSizes[i].first ; ++s) // first reverse export pairs
	AllReverseRecv.push_back(ReverseRecvBuffer[i][s]);

//    if(MyPID<4) std::cerr<<MyPID<<" after i="<<i<<"  ARR.size() "<<AllReverseRecv.size()<<std::endl;

    for(size_t s=ReverseRecvSizes[i].first ; s<ReverseRecvSizes[i].first+ReverseRecvSizes[i].second; ++s) { 
	fromRemoteGID.push_back(ReverseRecvBuffer[i][s]);

	if(ReverseRecvBuffer[i][s].first != ProcsTo[i]) {  
	    errstr <<MyPID<<"E6 error in unpack RecvBuffPID "<<ReverseRecvBuffer[i][s].first<<" from "<<ProcsTo[i]<<" for buff index "<<i<<" of "<<ReverseRecvBuffer.size()<< std::endl;
	    error=true;
	}

    }
  }

  if(AllReverseRecv.size() !=totalexportpairrecsize) {
      errstr <<MyPID<<"E7 error in unpack RecvBuffPID AllReverseRecv.size!=totalexportpairrecsize "<<AllReverseRecv.size()<<" vs "<<totalexportpairrecsize<<std::endl;
      error=true;
  }
  
  if(fromRemoteGID.size() !=totalforeignremotesize) {
     errstr <<MyPID<<"E8 error in unpack RecvBuffPID fromRemoteGID.size!=totalforeignremotesize "<<fromRemoteGID.size()<<" vs "<<totalforeignremotesize<<std::endl;
     error=true;
  }
  

  std::sort(AllReverseRecv.begin(), AllReverseRecv.end(), Tpetra::Import_Util::sort_PID_then_GID<int, GlobalOrdinal>);
//  std::sort(AllReverseRecv.begin(), AllReverseRecv.end());
  auto newEndOfPairs = std::unique(AllReverseRecv.begin(), AllReverseRecv.end());
  AllReverseRecv.resize( std::distance(AllReverseRecv.begin(),newEndOfPairs) );
  reversePIDs.clear();
  reverseLIDs.clear();
  reverseGIDs.clear();
  if(AllReverseRecv.size() == 0) {
    return;
  }
  reversePIDs.reserve(AllReverseRecv.size());
  reverseLIDs.reserve(AllReverseRecv.size());
  reverseGIDs.reserve(AllReverseRecv.size());

  for(auto && pgPair : AllReverseRecv) {
    if((int)pgPair.first != MyPID) {
      // if this pair is not found in fromRemoteGID, it's bogus, skip it. 
      // if(std::find(fromRemoteGID.begin(),fromRemoteGID.end(),pgPair) == fromRemoteGID.end()) 
      //   {
      //     std::ostringstream os;
      //     os<<MyPID<<"  RND not found "<<pgPair.first<<" "<<pgPair.second<<std::endl;
      //     std::cerr<<os.str()<<std::flush;
      //     continue;
      //   }
      reversePIDs.push_back((int)pgPair.first);
      reverseGIDs.push_back(pgPair.second);
      LocalOrdinal lid = MyDomainMap->getLocalElement(pgPair.second);
      reverseLIDs.push_back(lid);
    }
  }

  if(error){
      std::cerr<<errstr.str()<<std::flush;
      comm->barrier();
      comm->barrier();
      comm->barrier();
      MPI_Abort (MPI_COMM_WORLD, -1);
  }
}

// Note: This should get merged with the other Tpetra sort routines eventually.
template<typename Scalar, typename Ordinal>
void
sortCrsEntries (const Teuchos::ArrayView<size_t> &CRS_rowptr,
                const Teuchos::ArrayView<Ordinal> & CRS_colind,
                const Teuchos::ArrayView<Scalar> &CRS_vals)
{
  // For each row, sort column entries from smallest to largest.
  // Use shell sort. Stable sort so it is fast if indices are already sorted.
  // Code copied from  Epetra_CrsMatrix::SortEntries()
  size_t NumRows = CRS_rowptr.size()-1;
  size_t nnz = CRS_colind.size();

  const bool permute_values_array = CRS_vals.size() > 0;

  for(size_t i = 0; i < NumRows; i++){
    size_t start=CRS_rowptr[i];
    if(start >= nnz) continue;

    size_t NumEntries   = CRS_rowptr[i+1] - start;
    Teuchos::ArrayRCP<Scalar> locValues;
    if (permute_values_array)
      locValues = Teuchos::arcp<Scalar>(&CRS_vals[start], 0, NumEntries, false);
    Teuchos::ArrayRCP<Ordinal> locIndices(&CRS_colind[start], 0, NumEntries, false);

    Ordinal n = NumEntries;
    Ordinal m = 1;
    while (m<n) m = m*3+1;
    m /= 3;

    while(m > 0) {
      Ordinal max = n - m;
      for(Ordinal j = 0; j < max; j++) {
        for(Ordinal k = j; k >= 0; k-=m) {
          if(locIndices[k+m] >= locIndices[k])
            break;
          if (permute_values_array) {
            Scalar dtemp = locValues[k+m];
            locValues[k+m] = locValues[k];
            locValues[k] = dtemp;
          }
          Ordinal itemp = locIndices[k+m];
          locIndices[k+m] = locIndices[k];
          locIndices[k] = itemp;
        }
      }
      m = m/3;
    }
  }
}

template<typename Ordinal>
void
sortCrsEntries (const Teuchos::ArrayView<size_t> &CRS_rowptr,
                const Teuchos::ArrayView<Ordinal> & CRS_colind)
{
  // Generate dummy values array
  Teuchos::ArrayView<Tpetra::Details::DefaultTypes::scalar_type> CRS_vals;
  sortCrsEntries (CRS_rowptr, CRS_colind, CRS_vals);
}

namespace Impl {

template<class RowOffsetsType, class ColumnIndicesType, class ValuesType>
class SortCrsEntries {
private:
  typedef typename ColumnIndicesType::non_const_value_type ordinal_type;
  typedef typename ValuesType::non_const_value_type scalar_type;

public:
  SortCrsEntries (const RowOffsetsType& ptr,
                  const ColumnIndicesType& ind,
                  const ValuesType& val) :
    ptr_ (ptr),
    ind_ (ind),
    val_ (val)
  {
    static_assert (std::is_signed<ordinal_type>::value, "The type of each "
                   "column index -- that is, the type of each entry of ind "
                   "-- must be signed in order for this functor to work.");
  }

  KOKKOS_FUNCTION void operator() (const size_t i) const
  {
    const size_t nnz = ind_.extent (0);
    const size_t start = ptr_(i);
    const bool permute_values_array = val_.extent(0) > 0;

    if (start < nnz) {
      const size_t NumEntries = ptr_(i+1) - start;

      const ordinal_type n = static_cast<ordinal_type> (NumEntries);
      ordinal_type m = 1;
      while (m<n) m = m*3+1;
      m /= 3;

      while (m > 0) {
        ordinal_type max = n - m;
        for (ordinal_type j = 0; j < max; j++) {
          for (ordinal_type k = j; k >= 0; k -= m) {
            const size_t sk = start+k;
            if (ind_(sk+m) >= ind_(sk)) {
              break;
            }
            if (permute_values_array) {
              const scalar_type dtemp = val_(sk+m);
              val_(sk+m)   = val_(sk);
              val_(sk)     = dtemp;
            }
            const ordinal_type itemp = ind_(sk+m);
            ind_(sk+m) = ind_(sk);
            ind_(sk)   = itemp;
          }
        }
        m = m/3;
      }
    }
  }

  static void
  sortCrsEntries (const RowOffsetsType& ptr,
                  const ColumnIndicesType& ind,
                  const ValuesType& val)
  {
    // For each row, sort column entries from smallest to largest.
    // Use shell sort. Stable sort so it is fast if indices are already sorted.
    // Code copied from  Epetra_CrsMatrix::SortEntries()
    // NOTE: This should not be taken as a particularly efficient way to sort
    // rows of matrices in parallel.  But it is correct, so that's something.
    if (ptr.extent (0) == 0) {
      return; // no rows, so nothing to sort
    }
    const size_t NumRows = ptr.extent (0) - 1;

    typedef size_t index_type; // what this function was using; not my choice
    typedef typename ValuesType::execution_space execution_space;
    // Specify RangePolicy explicitly, in order to use correct execution
    // space.  See #1345.
    typedef Kokkos::RangePolicy<execution_space, index_type> range_type;
    Kokkos::parallel_for ("sortCrsEntries", range_type (0, NumRows),
      SortCrsEntries (ptr, ind, val));
  }

private:
  RowOffsetsType ptr_;
  ColumnIndicesType ind_;
  ValuesType val_;
};

} // namespace Impl

template<typename rowptr_array_type, typename colind_array_type, typename vals_array_type>
void
sortCrsEntries (const rowptr_array_type& CRS_rowptr,
                const colind_array_type& CRS_colind,
                const vals_array_type& CRS_vals)
{
  Impl::SortCrsEntries<rowptr_array_type, colind_array_type,
    vals_array_type>::sortCrsEntries (CRS_rowptr, CRS_colind, CRS_vals);
}

template<typename rowptr_array_type, typename colind_array_type>
void
sortCrsEntries (const rowptr_array_type& CRS_rowptr,
                const colind_array_type& CRS_colind)
{
  // Generate dummy values array
  typedef typename colind_array_type::execution_space execution_space;
  typedef Tpetra::Details::DefaultTypes::scalar_type scalar_type;
  typedef typename Kokkos::View<scalar_type*, execution_space> scalar_view_type;
  scalar_view_type CRS_vals;
  sortCrsEntries<rowptr_array_type, colind_array_type,
    scalar_view_type>(CRS_rowptr, CRS_colind, CRS_vals);
}

// Note: This should get merged with the other Tpetra sort routines eventually.
template<typename Scalar, typename Ordinal>
void
sortAndMergeCrsEntries (const Teuchos::ArrayView<size_t> &CRS_rowptr,
                        const Teuchos::ArrayView<Ordinal> & CRS_colind,
                        const Teuchos::ArrayView<Scalar> &CRS_vals)
{
  // For each row, sort column entries from smallest to largest,
  // merging column ids that are identify by adding values.  Use shell
  // sort. Stable sort so it is fast if indices are already sorted.
  // Code copied from Epetra_CrsMatrix::SortEntries()

  if (CRS_rowptr.size () == 0) {
    return; // no rows, so nothing to sort
  }
  const size_t NumRows = CRS_rowptr.size () - 1;
  const size_t nnz = CRS_colind.size ();
  size_t new_curr = CRS_rowptr[0];
  size_t old_curr = CRS_rowptr[0];

  const bool permute_values_array = CRS_vals.size() > 0;

  for(size_t i = 0; i < NumRows; i++){
    const size_t old_rowptr_i=CRS_rowptr[i];
    CRS_rowptr[i] = old_curr;
    if(old_rowptr_i >= nnz) continue;

    size_t NumEntries   = CRS_rowptr[i+1] - old_rowptr_i;
    Teuchos::ArrayRCP<Scalar> locValues;
    if (permute_values_array)
      locValues = Teuchos::arcp<Scalar>(&CRS_vals[old_rowptr_i], 0, NumEntries, false);
    Teuchos::ArrayRCP<Ordinal> locIndices(&CRS_colind[old_rowptr_i], 0, NumEntries, false);

    // Sort phase
    Ordinal n = NumEntries;
    Ordinal m = n/2;

    while(m > 0) {
      Ordinal max = n - m;
      for(Ordinal j = 0; j < max; j++) {
        for(Ordinal k = j; k >= 0; k-=m) {
          if(locIndices[k+m] >= locIndices[k])
            break;
          if (permute_values_array) {
            Scalar dtemp = locValues[k+m];
            locValues[k+m] = locValues[k];
            locValues[k] = dtemp;
          }
          Ordinal itemp = locIndices[k+m];
          locIndices[k+m] = locIndices[k];
          locIndices[k] = itemp;
        }
      }
      m = m/2;
    }

    // Merge & shrink
    for(size_t j=old_rowptr_i; j < CRS_rowptr[i+1]; j++) {
      if(j > old_rowptr_i && CRS_colind[j]==CRS_colind[new_curr-1]) {
        if (permute_values_array) CRS_vals[new_curr-1] += CRS_vals[j];
      }
      else if(new_curr==j) {
        new_curr++;
      }
      else {
        CRS_colind[new_curr] = CRS_colind[j];
        if (permute_values_array) CRS_vals[new_curr]   = CRS_vals[j];
        new_curr++;
      }
    }
    old_curr=new_curr;
  }

  CRS_rowptr[NumRows] = new_curr;
}

template<typename Ordinal>
void
sortAndMergeCrsEntries (const Teuchos::ArrayView<size_t> &CRS_rowptr,
                        const Teuchos::ArrayView<Ordinal> & CRS_colind)
{
  Teuchos::ArrayView<Tpetra::Details::DefaultTypes::scalar_type> CRS_vals;
  return sortAndMergeCrsEntries(CRS_rowptr, CRS_colind, CRS_vals);
}


template <typename LocalOrdinal, typename GlobalOrdinal, typename Node>
void
lowCommunicationMakeColMapAndReindex (const Teuchos::ArrayView<const size_t> &rowptr,
                                      const Teuchos::ArrayView<LocalOrdinal> &colind_LID,
                                      const Teuchos::ArrayView<GlobalOrdinal> &colind_GID,
                                      const Teuchos::RCP<const Tpetra::Map<LocalOrdinal,GlobalOrdinal,Node> >& domainMapRCP,
                                      const Teuchos::ArrayView<const int> &owningPIDs,
                                      Teuchos::Array<int> &remotePIDs,
                                      Teuchos::RCP<const Tpetra::Map<LocalOrdinal,GlobalOrdinal,Node> > & colMap)
{
  using Teuchos::rcp;
  typedef LocalOrdinal LO;
  typedef GlobalOrdinal GO;
  typedef Tpetra::global_size_t GST;
  typedef Tpetra::Map<LO, GO, Node> map_type;
  const char prefix[] = "lowCommunicationMakeColMapAndReindex: ";

  // The domainMap is an RCP because there is a shortcut for a
  // (common) special case to return the columnMap = domainMap.
  const map_type& domainMap = *domainMapRCP;

  // Scan all column indices and sort into two groups:
  // Local:  those whose GID matches a GID of the domain map on this processor and
  // Remote: All others.
  const size_t numDomainElements = domainMap.getNodeNumElements ();
  Teuchos::Array<bool> LocalGIDs;
  if (numDomainElements > 0) {
    LocalGIDs.resize (numDomainElements, false); // Assume domain GIDs are not local
  }

  // In principle it is good to have RemoteGIDs and RemotGIDList be as
  // long as the number of remote GIDs on this processor, but this
  // would require two passes through the column IDs, so we make it
  // the max of 100 and the number of block rows.
  //
  // FIXME (mfh 11 Feb 2015) Tpetra::Details::HashTable can hold at
  // most INT_MAX entries, but it's possible to have more rows than
  // that (if size_t is 64 bits and int is 32 bits).
  const size_t numMyRows = rowptr.size () - 1;
  const int hashsize = std::max (static_cast<int> (numMyRows), 100);

  Tpetra::Details::HashTable<GO, LO> RemoteGIDs (hashsize);
  Teuchos::Array<GO> RemoteGIDList;
  RemoteGIDList.reserve (hashsize);
  Teuchos::Array<int> PIDList;
  PIDList.reserve (hashsize);

  // Here we start using the *LocalOrdinal* colind_LID array.  This is
  // safe even if both columnIndices arrays are actually the same
  // (because LocalOrdinal==GO).  For *local* GID's set
  // colind_LID with with their LID in the domainMap.  For *remote*
  // GIDs, we set colind_LID with (numDomainElements+NumRemoteColGIDs)
  // before the increment of the remote count.  These numberings will
  // be separate because no local LID is greater than
  // numDomainElements.

  size_t NumLocalColGIDs = 0;
  LO NumRemoteColGIDs = 0;
  for (size_t i = 0; i < numMyRows; ++i) {
    for(size_t j = rowptr[i]; j < rowptr[i+1]; ++j) {
      const GO GID = colind_GID[j];
      // Check if GID matches a row GID
      const LO LID = domainMap.getLocalElement (GID);
      if(LID != -1) {
        const bool alreadyFound = LocalGIDs[LID];
        if (! alreadyFound) {
          LocalGIDs[LID] = true; // There is a column in the graph associated with this domain map GID
          NumLocalColGIDs++;
        }
        colind_LID[j] = LID;
      }
      else {
        const LO hash_value = RemoteGIDs.get (GID);
        if (hash_value == -1) { // This means its a new remote GID
          const int PID = owningPIDs[j];
          TEUCHOS_TEST_FOR_EXCEPTION(
            PID == -1, std::invalid_argument, prefix << "Cannot figure out if "
            "PID is owned.");
          colind_LID[j] = static_cast<LO> (numDomainElements + NumRemoteColGIDs);
          RemoteGIDs.add (GID, NumRemoteColGIDs);
          RemoteGIDList.push_back (GID);
          PIDList.push_back (PID);
          NumRemoteColGIDs++;
        }
        else {
          colind_LID[j] = static_cast<LO> (numDomainElements + hash_value);
        }
      }
    }
  }

  // Possible short-circuit: If all domain map GIDs are present as
  // column indices, then set ColMap=domainMap and quit.
  if (domainMap.getComm ()->getSize () == 1) {
    // Sanity check: When there is only one process, there can be no
    // remoteGIDs.
    TEUCHOS_TEST_FOR_EXCEPTION(
      NumRemoteColGIDs != 0, std::runtime_error, prefix << "There is only one "
      "process in the domain Map's communicator, which means that there are no "
      "\"remote\" indices.  Nevertheless, some column indices are not in the "
      "domain Map.");
    if (static_cast<size_t> (NumLocalColGIDs) == numDomainElements) {
      // In this case, we just use the domainMap's indices, which is,
      // not coincidently, what we clobbered colind with up above
      // anyway.  No further reindexing is needed.
      colMap = domainMapRCP;
      return;
    }
  }

  // Now build the array containing column GIDs
  // Build back end, containing remote GIDs, first
  const LO numMyCols = NumLocalColGIDs + NumRemoteColGIDs;
  Teuchos::Array<GO> ColIndices;
  GO* RemoteColIndices = NULL;
  if (numMyCols > 0) {
    ColIndices.resize (numMyCols);
    if (NumLocalColGIDs != static_cast<size_t> (numMyCols)) {
      RemoteColIndices = &ColIndices[NumLocalColGIDs]; // Points to back half of ColIndices
    }
  }

  for (LO i = 0; i < NumRemoteColGIDs; ++i) {
    RemoteColIndices[i] = RemoteGIDList[i];
  }

  // Build permute array for *remote* reindexing.
  Teuchos::Array<LO> RemotePermuteIDs (NumRemoteColGIDs);
  for (LO i = 0; i < NumRemoteColGIDs; ++i) {
    RemotePermuteIDs[i]=i;
  }

  // Sort External column indices so that all columns coming from a
  // given remote processor are contiguous.  This is a sort with two
  // auxilary arrays: RemoteColIndices and RemotePermuteIDs.
  Tpetra::sort3 (PIDList.begin (), PIDList.end (),
                 ColIndices.begin () + NumLocalColGIDs,
                 RemotePermuteIDs.begin ());

  // Stash the RemotePIDs.
  //
  // Note: If Teuchos::Array had a shrink_to_fit like std::vector,
  // we'd call it here.
  remotePIDs = PIDList;

  // Sort external column indices so that columns from a given remote
  // processor are not only contiguous but also in ascending
  // order. NOTE: I don't know if the number of externals associated
  // with a given remote processor is known at this point ... so I
  // count them here.

  // NTS: Only sort the RemoteColIndices this time...
  LO StartCurrent = 0, StartNext = 1;
  while (StartNext < NumRemoteColGIDs) {
    if (PIDList[StartNext]==PIDList[StartNext-1]) {
      StartNext++;
    }
    else {
      Tpetra::sort2 (ColIndices.begin () + NumLocalColGIDs + StartCurrent,
                     ColIndices.begin () + NumLocalColGIDs + StartNext,
                     RemotePermuteIDs.begin () + StartCurrent);
      StartCurrent = StartNext;
      StartNext++;
    }
  }
  Tpetra::sort2 (ColIndices.begin () + NumLocalColGIDs + StartCurrent,
                 ColIndices.begin () + NumLocalColGIDs + StartNext,
                 RemotePermuteIDs.begin () + StartCurrent);

  // Reverse the permutation to get the information we actually care about
  Teuchos::Array<LO> ReverseRemotePermuteIDs (NumRemoteColGIDs);
  for (LO i = 0; i < NumRemoteColGIDs; ++i) {
    ReverseRemotePermuteIDs[RemotePermuteIDs[i]] = i;
  }

  // Build permute array for *local* reindexing.
  bool use_local_permute = false;
  Teuchos::Array<LO> LocalPermuteIDs (numDomainElements);

  // Now fill front end. Two cases:
  //
  // (1) If the number of Local column GIDs is the same as the number
  //     of Local domain GIDs, we can simply read the domain GIDs into
  //     the front part of ColIndices, otherwise
  //
  // (2) We step through the GIDs of the domainMap, checking to see if
  //     each domain GID is a column GID.  we want to do this to
  //     maintain a consistent ordering of GIDs between the columns
  //     and the domain.
  Teuchos::ArrayView<const GO> domainGlobalElements = domainMap.getNodeElementList();
  if (static_cast<size_t> (NumLocalColGIDs) == numDomainElements) {
    if (NumLocalColGIDs > 0) {
      // Load Global Indices into first numMyCols elements column GID list
      std::copy (domainGlobalElements.begin (), domainGlobalElements.end (),
                 ColIndices.begin ());
    }
  }
  else {
    LO NumLocalAgain = 0;
    use_local_permute = true;
    for (size_t i = 0; i < numDomainElements; ++i) {
      if (LocalGIDs[i]) {
        LocalPermuteIDs[i] = NumLocalAgain;
        ColIndices[NumLocalAgain++] = domainGlobalElements[i];
      }
    }
    TEUCHOS_TEST_FOR_EXCEPTION(
      static_cast<size_t> (NumLocalAgain) != NumLocalColGIDs,
      std::runtime_error, prefix << "Local ID count test failed.");
  }

  // Make column Map
  const GST minus_one = Teuchos::OrdinalTraits<GST>::invalid ();
  colMap = rcp (new map_type (minus_one, ColIndices, domainMap.getIndexBase (),
                              domainMap.getComm (), domainMap.getNode ()));

  // Low-cost reindex of the matrix
  for (size_t i = 0; i < numMyRows; ++i) {
    for (size_t j = rowptr[i]; j < rowptr[i+1]; ++j) {
      const LO ID = colind_LID[j];
      if (static_cast<size_t> (ID) < numDomainElements) {
        if (use_local_permute) {
          colind_LID[j] = LocalPermuteIDs[colind_LID[j]];
        }
        // In the case where use_local_permute==false, we just copy
        // the DomainMap's ordering, which it so happens is what we
        // put in colind_LID to begin with.
      }
      else {
        colind_LID[j] =  NumLocalColGIDs + ReverseRemotePermuteIDs[colind_LID[j]-numDomainElements];
      }
    }
  }
  if(remotePIDs.size() != RemoteGIDList.size() || NumRemoteColGIDs !=  RemoteGIDList.size()) {
    std::ostringstream os;
    os<<" Size missmatch in lowCommunicationMakeColMapAndReindex "<<remotePIDs.size()<<" "<<RemoteGIDList.size()<<" "<<NumRemoteColGIDs <<std::endl;
    std::cerr<<os.str()<<std::flush;
  }
  

}




// Generates an list of owning PIDs based on two transfer (aka import/export objects)
// Let:
//   OwningMap = useReverseModeForOwnership ? transferThatDefinesOwnership.getTargetMap() : transferThatDefinesOwnership.getSourceMap();
//   MapAo     = useReverseModeForOwnership ? transferThatDefinesOwnership.getSourceMap() : transferThatDefinesOwnership.getTargetMap();
//   MapAm     = useReverseModeForMigration ? transferThatDefinesMigration.getTargetMap() : transferThatDefinesMigration.getSourceMap();
//   VectorMap = useReverseModeForMigration ? transferThatDefinesMigration.getSourceMap() : transferThatDefinesMigration.getTargetMap();
// Precondition:
//  1) MapAo.isSameAs(*MapAm)                      - map compatibility between transfers
//  2) VectorMap->isSameAs(*owningPIDs->getMap())  - map compabibility between transfer & vector
//  3) OwningMap->isOneToOne()                     - owning map is 1-to-1
//  --- Precondition 3 is only checked in DEBUG mode ---
// Postcondition:
//   owningPIDs[VectorMap->getLocalElement(GID i)] =   j iff  (OwningMap->isLocalElement(GID i) on rank j)
template <typename LocalOrdinal, typename GlobalOrdinal, typename Node>
void getTwoTransferOwnershipVector(const ::Tpetra::Details::Transfer<LocalOrdinal, GlobalOrdinal, Node>& transferThatDefinesOwnership,
                                   bool useReverseModeForOwnership,
                                   const ::Tpetra::Details::Transfer<LocalOrdinal, GlobalOrdinal, Node>& transferThatDefinesMigration,
                                   bool useReverseModeForMigration,
                                   Tpetra::Vector<int,LocalOrdinal,GlobalOrdinal,Node> & owningPIDs) {
  typedef Tpetra::Import<LocalOrdinal, GlobalOrdinal, Node> import_type;
  typedef Tpetra::Export<LocalOrdinal, GlobalOrdinal, Node> export_type;

  Teuchos::RCP<const Tpetra::Map<LocalOrdinal,GlobalOrdinal,Node> > OwningMap = useReverseModeForOwnership ? 
                                                                                transferThatDefinesOwnership.getTargetMap() : 
                                                                                transferThatDefinesOwnership.getSourceMap();
  Teuchos::RCP<const Tpetra::Map<LocalOrdinal,GlobalOrdinal,Node> > MapAo     = useReverseModeForOwnership ? 
                                                                                transferThatDefinesOwnership.getSourceMap() : 
                                                                                transferThatDefinesOwnership.getTargetMap();
  Teuchos::RCP<const Tpetra::Map<LocalOrdinal,GlobalOrdinal,Node> > MapAm     = useReverseModeForMigration ? 
                                                                                transferThatDefinesMigration.getTargetMap() : 
                                                                                transferThatDefinesMigration.getSourceMap();
  Teuchos::RCP<const Tpetra::Map<LocalOrdinal,GlobalOrdinal,Node> > VectorMap = useReverseModeForMigration ? 
                                                                                transferThatDefinesMigration.getSourceMap() : 
                                                                                transferThatDefinesMigration.getTargetMap();

  TEUCHOS_TEST_FOR_EXCEPTION(!MapAo->isSameAs(*MapAm),std::runtime_error,"Tpetra::Import_Util::getTwoTransferOwnershipVector map mismatch between transfers");
  TEUCHOS_TEST_FOR_EXCEPTION(!VectorMap->isSameAs(*owningPIDs.getMap()),std::runtime_error,"Tpetra::Import_Util::getTwoTransferOwnershipVector map mismatch transfer and vector");
#ifdef HAVE_TPETRA_DEBUG
  TEUCHOS_TEST_FOR_EXCEPTION(!OwningMap->isOneToOne(),std::runtime_error,"Tpetra::Import_Util::getTwoTransferOwnershipVector owner must be 1-to-1");
#endif

  int rank = OwningMap->getComm()->getRank();
  // Generate "A" vector and fill it with owning information.  We can read this from transferThatDefinesOwnership w/o communication
  // Note:  Due to the 1-to-1 requirement, several of these options throw
  Tpetra::Vector<int,LocalOrdinal,GlobalOrdinal,Node> temp(MapAo);
  const import_type* ownAsImport = dynamic_cast<const import_type*> (&transferThatDefinesOwnership);
  const export_type* ownAsExport = dynamic_cast<const export_type*> (&transferThatDefinesOwnership);

  Teuchos::ArrayRCP<int> pids    = temp.getDataNonConst();
  Teuchos::ArrayView<int> v_pids = pids();
  if(ownAsImport && useReverseModeForOwnership)       {TEUCHOS_TEST_FOR_EXCEPTION(1,std::runtime_error,"Tpetra::Import_Util::getTwoTransferOwnershipVector owner must be 1-to-1");}
  else if(ownAsImport && !useReverseModeForOwnership) getPids(*ownAsImport,v_pids,false);
  else if(ownAsExport && useReverseModeForMigration)  {TEUCHOS_TEST_FOR_EXCEPTION(1,std::runtime_error,"Tpetra::Import_Util::getTwoTransferOwnershipVector this option not yet implemented");}
  else                                                {TEUCHOS_TEST_FOR_EXCEPTION(1,std::runtime_error,"Tpetra::Import_Util::getTwoTransferOwnershipVector owner must be 1-to-1");}

  const import_type* xferAsImport = dynamic_cast<const import_type*> (&transferThatDefinesMigration);
  const export_type* xferAsExport = dynamic_cast<const export_type*> (&transferThatDefinesMigration);
  TEUCHOS_TEST_FOR_EXCEPTION(!xferAsImport && !xferAsExport,std::runtime_error,"Tpetra::Import_Util::getTwoTransferOwnershipVector transfer undefined");

  // Migrate from "A" vector to output vector
  owningPIDs.putScalar(rank);
  if(xferAsImport && useReverseModeForMigration)        owningPIDs.doExport(temp,*xferAsImport,Tpetra::REPLACE);
  else if(xferAsImport && !useReverseModeForMigration)  owningPIDs.doImport(temp,*xferAsImport,Tpetra::REPLACE);
  else if(xferAsExport && useReverseModeForMigration)   owningPIDs.doImport(temp,*xferAsExport,Tpetra::REPLACE);
  else                                                  owningPIDs.doExport(temp,*xferAsExport,Tpetra::REPLACE);

}



} // namespace Import_Util
} // namespace Tpetra

#endif // TPETRA_IMPORT_UTIL_HPP
